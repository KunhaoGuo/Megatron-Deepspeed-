# 参数量

transformer模型由 l 个相同的层组成，每层分为两个部分：self- attention块和MLP块。

self-attention块的模型参数有Q、K、V的权重矩阵W~Q~ 、W~K~、 W~V~ 和偏置，输出权重矩阵W~O~ 和偏置。所以self- attention块的模型参数量是 4h^2^ + 4h。

MLP有两个线性层组成，通常第一个线性层将维度从 h 映射到 4h ，第二个线性层将维度从 4h 映射到 h。第一个线性层和第二个线性层偏置的形状分别为[4h]、[h]，所以MLP块的模型参数量是 8h^2^ + 5h。

self- attention块和MLP块各有一个layer- normalization，包含两个可训练的参数：缩放参数$\gamma$ 和平移参数$$\beta$$ ，形状都是 [h]。所以2个layer normalization的参数量是 4h 。

输出层参数矩阵和嵌入层参数矩阵一般是共享的，所以输出层参数量是 Vh 。

位置编码的模型参数量是 Sh （S是最大编码长度）。

所以总的模型参数量是 $l * (12h^2+13h)+(V+S)h=12lh^2(1+\frac{13}{12h}+\frac{V+S}{12lh})$

h 足够大时，模型参数量大约为 $12lh^2$。

# 计算量

对于$A\in R^{M*N}$，$B\in R^{N*P}$，计算 AB 需要的浮点数运算次数为 2mnp。

只考虑矩阵乘法的计算量。

**self- attention块**

1. 计算Q、K、V：$[b,s,h] * [h,h]\rightarrow [b,s,h]$。计算量为$3 * 2bsh^2$。
2. 计算注意力分数：$[b,head_num,s,per_head_hidden_size]*[b,head_num,per_head_hidden_size,s] \rightarrow [b, head_num, s,s] $ ，计算量为$2bs^2h$。
3. score * V：$[b,head_num,s,s]*[b,head_num,s,per_head_hidden_size] \rightarrow [b,head_num,s,per_head_hidden_size]$，计算量为$2bs^2h$。
4. attention后的线性映射：$[b,s,h]*[h,h]\rightarrow[b,s,h]$，计算量为$2bsh^2$ 。

**MLP块**

1. 第一个线性层：$[b,s,h]*[h,4h]\rightarrow[b,s,4h]$ ，计算量为$8bsh^2$ 。
2. 第二个线性层：$[b,s,4h]*[4h,h]\rightarrow[b,s,h]$ ，计算量为$8bsh^2$ 。

logits的计算：$[b,s,h]*[h,V]\rightarrow[b,s,V]$ ，计算量为$2bshV$ 。

反向传播是前向传播计算量的 2 倍，因为要分别计算输入和权重的梯度。

如果使用激活重计算，在反向传播前需要额外进行一次前向计算。

对于有 l 层的transformer模型，一次iteration需要的浮点数运算次数为

$4*l*((24bsh^2 + 4bs^2h))+6bshV = 96bslh^2(1+\frac{s}{6h}+\frac{v}{16lh})$

当 h 足够大时，可近似为$96bslh^2$ 。

# 参数量和计算量的关系

一次iteration输入的token数是 bs ，$\frac{96bslh^2}{12lh^2*bs}=8$，所以在一次iteration中，对于每个参数，每个token，需要8次浮点数运算。

## Activation Memory

​	"activation"是指，在前向传播过程中形成，在反向传播传播过程中的梯度计算所需要的所有tensor。不包括模型参数和优化器状态，但是包括dropout操作所使用的mask。只考虑主要的内存占用，会忽略一些小的内存缓冲，例如对于layer normalization，为了计算梯度，需要保存其输入，输入的均值和方差，所需要保存的数据量分别是 bsh、bs、bs，由于 h 远大于 s，所以 bs 可以忽略。我们假设网络中的激活值以16位浮点数的形式存储，所以每个数据需要2字节存储，但对于mask只需要1字节存储。以下描述的内存占用以字节为单位。

![transformer](/Users/guokunhao/笔记/数据量计算/transformer.png)

**Attention Block**

1. Layer norm：需要保存输入：2bsh
2. Q、K、V矩阵乘法：需要保存它们的共享输入：2sbh
3. QK^T^ 矩阵乘法：需要分别保存Q和K：2sbh、2sbh
4. softmax：需要保存softmax的输出：2as^2^b（a 是 head_num）
5. softmax dropout：需要保存max：as^2^b
6. attention over values(V)：需要保存dropout的输出和V：2as^2^b、2sbh
7. linear project：需要保存输入激活值（attention over values的输出）：2sbh
8. attention dropout：需要保存mask：sbh

所以attention- block需要保存的大小为：13sbh + 5as^2^b

**MLP**

1. layer norm：需要保存输入：2sbh
2. 第一个线性层的输入：2sbh
3. GeLU非线性层的输入：8sbh
4. 第二个线性层的输入：8sbh
5. linear dropout的mask：sbh

所以MLP- block需要保存的大小为：21sbh

**对于transformer的一个layer需要保存的激活内存占用为：$sbh(34+5\frac{as}{h})$**

